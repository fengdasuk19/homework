{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the condition where epoch = 100, compare three kinds of convnets using different training ways:\n",
    "\n",
    "1. Firstly train digits 0~4, then 5~9\n",
    "2. Firstly train digits 0~4, then 0~9\n",
    "3. Firstly train digits 0~9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare way:\n",
    "\n",
    "1. curves' shape\n",
    "2. accuracy on test sets\n",
    "3. **[Note]** Conclusions must be based on means of results from multiple experients, 5 at least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should commit:\n",
    "\n",
    "1. Source code\n",
    "2. A PDF report with details of experiments, including details of methods, results in graphs and conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import modules\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some constant variables\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "dtype_float = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "dtype_long = torch.cuda.LongTensor if use_cuda else torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# prepare MNIST\n",
    "\n",
    "path_datasets = './datasets'\n",
    "\n",
    "transform_set = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "data_train = torchvision.datasets.MNIST(root=path_datasets, train=True, download=True, \n",
    "                                        transform=transform_set) \n",
    "data_test = torchvision.datasets.MNIST(root=path_datasets, train=False, download=True,\n",
    "                                        transform=transform_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_class = 10\n",
    "\n",
    "##dataloader_train\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset=data_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "##dataloader_valid\n",
    "\n",
    "indices_valid = range(len(data_test)//2)\n",
    "sampler_valid = torch.utils.data.sampler.SubsetRandomSampler(indices=indices_valid)\n",
    "dataloader_valid = torch.utils.data.DataLoader(dataset=data_test, batch_size=batch_size, sampler=sampler_valid)\n",
    "\n",
    "##dataloader_test\n",
    "\n",
    "indices_test = range(len(data_test)//2, len(data_test))\n",
    "sampler_test = torch.utils.data.sampler.SubsetRandomSampler(indices=indices_test)\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset=data_test, batch_size=batch_size, sampler=sampler_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看三个数据迭代器的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0  5923\n",
       "1  6742\n",
       "2  5958\n",
       "3  6131\n",
       "4  5842\n",
       "5  5421\n",
       "6  5918\n",
       "7  6265\n",
       "8  5851\n",
       "9  5949"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEVFLEkdERNSSxBEREbX8f/VhCVKLBh52AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd15219550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd15a7b4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = {i:0 for i in range(n_class)}\n",
    "for i,data in enumerate(dataloader_train):\n",
    "    xs,ys = data\n",
    "    for y in ys:\n",
    "        count[y] += 1\n",
    "\n",
    "count = [count[i] for i in range(n_class)]\n",
    "display(pd.DataFrame(data=count))\n",
    "plt.bar(left=np.arange(n_class), height=count)\n",
    "plt.xlabel('digit classes')\n",
    "plt.ylabel('num of samples')\n",
    "plt.title('simple stat: data_train')\n",
    "plt.show()\n",
    "plt.savefig('stat-train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0  460\n",
       "1  571\n",
       "2  530\n",
       "3  500\n",
       "4  500\n",
       "5  456\n",
       "6  462\n",
       "7  512\n",
       "8  489\n",
       "9  520"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd15a730b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd15203860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = {i:0 for i in range(n_class)}\n",
    "for i,data in enumerate(dataloader_valid):\n",
    "    xs,ys = data\n",
    "    for y in ys:\n",
    "        count[y] += 1\n",
    "\n",
    "count = [count[i] for i in range(n_class)]\n",
    "display(pd.DataFrame(data=count))\n",
    "plt.bar(left=np.arange(n_class), height=count)\n",
    "plt.xlabel('digit classes')\n",
    "plt.ylabel('num of samples')\n",
    "plt.title('simple stat: data_valid')\n",
    "plt.show()\n",
    "plt.savefig('stat-validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0  520\n",
       "1  564\n",
       "2  502\n",
       "3  510\n",
       "4  482\n",
       "5  436\n",
       "6  496\n",
       "7  516\n",
       "8  485\n",
       "9  489"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd7809f0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd159eef98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = {i:0 for i in range(n_class)}\n",
    "for i,data in enumerate(dataloader_test):\n",
    "    xs,ys = data\n",
    "    for y in ys:\n",
    "        count[y] += 1\n",
    "\n",
    "count = [count[i] for i in range(n_class)]\n",
    "display(pd.DataFrame(data=count))\n",
    "plt.bar(left=np.arange(n_class), height=count)\n",
    "plt.xlabel('digit classes')\n",
    "plt.ylabel('num of samples')\n",
    "plt.title('simple stat: data_test')\n",
    "plt.show()\n",
    "plt.savefig('stat-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. ConvNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  torch.nn import functional as F\n",
    "\n",
    "n_channels = 1\n",
    "n_class = 10\n",
    "size_image = 28\n",
    "\n",
    "depth = [8, 16, 32]\n",
    "fc = [1024, 128, n_class]\n",
    "ksize = [5, 5, 5]\n",
    "pad = [2, 2, 2]\n",
    "\n",
    "net_names = []\n",
    "\n",
    "# define class\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, init_name):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # init\n",
    "        global net_names\n",
    "        assert init_name not in net_names, \"Name existed: {}\".format(net_names)\n",
    "        net_names.append(init_name)\n",
    "        self.name = init_name\n",
    "        \n",
    "        # conv-net\n",
    "        self.conv00 = nn.Conv2d(in_channels=n_channels, out_channels=depth[0], kernel_size=ksize[0], padding=pad[0])\n",
    "        self.conv01 = nn.Conv2d(in_channels=depth[0], out_channels=depth[1], kernel_size=ksize[1], padding=pad[1])\n",
    "        self.conv02 = nn.Conv2d(in_channels=depth[1], out_channels=depth[2], kernel_size=ksize[2], padding=pad[2])\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # full-connect\n",
    "        self.fc00 = nn.Linear(in_features=(size_image//4//2)*(size_image//4//2)*depth[2], out_features=fc[0])\n",
    "        self.fc01 = nn.Linear(in_features=fc[0], out_features=fc[1])\n",
    "        self.fc02 = nn.Linear(in_features=fc[1], out_features=fc[2])\n",
    "        \n",
    "        self.bn1d = nn.BatchNorm1d(num_features=fc[1])\n",
    "        \n",
    "    def rename(self, new_name):\n",
    "        global net_names\n",
    "        net_names.remove(self.name)\n",
    "        self.name = new_name\n",
    "        net_names.append(new_name)\n",
    "    \n",
    "    def forward(self, x, training=True):\n",
    "        x = F.leaky_relu(self.conv00(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.leaky_relu(self.conv01(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.leaky_relu(self.conv02(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(-1, (size_image//4//2)*(size_image//4//2)*depth[2])\n",
    "        \n",
    "        x = F.leaky_relu(self.fc00(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.leaky_relu(self.fc01(x))\n",
    "        x = self.bn1d(x)\n",
    "        x = F.leaky_relu(self.fc02(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaludate_acc(y, target):\n",
    "    if isinstance(target, torch.autograd.Variable):\n",
    "        target = target.cpu().data if use_cuda else target.data\n",
    "    if isinstance(y, torch.autograd.Variable):\n",
    "        y = y.cpu().data if use_cuda else y.data\n",
    "    pred = torch.max(y, dim=1)[-1]\n",
    "    rights = pred.eq(target).sum()\n",
    "    length = y.size()[0]\n",
    "    acc = 1.0 * rights / length\n",
    "    return (acc, rights, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define training function using specified digits\n",
    "\n",
    "\n",
    "def train_net(net, loss_func, optimizer, digit_range, sum_epochs=15, interval_valid=50):\n",
    "    bag_loss = {'train':[], 'valid':[]}\n",
    "    bag_acc = {'train':[], 'valid':[]}\n",
    "    for epoch in range(sum_epochs):\n",
    "        for idx,batch in enumerate(dataloader_train):\n",
    "            # set net state\n",
    "            net.train()\n",
    "\n",
    "            # fetch batch for this round, then choose points that are in `digit_range`\n",
    "            xs_raw, ys_raw = batch\n",
    "            xs, ys = [], []\n",
    "            for (data,label) in zip(xs_raw, ys_raw):\n",
    "                if label in digit_range:\n",
    "                    xs.append(data.unsqueeze(0))\n",
    "                    ys.append(label)\n",
    "            xs = torch.cat(xs)\n",
    "            ys = torch.from_numpy(np.array(ys))\n",
    "            xs, ys = (xs.cuda(), ys.cuda()) if use_cuda else (xs, ys)\n",
    "            \n",
    "            # pack in Variable\n",
    "            xs, ys = Variable(xs), Variable(ys)  # easy to forget packing `ys` in `Variable`\n",
    "\n",
    "            # predictions\n",
    "            preds = net(xs)\n",
    "\n",
    "            # count loss\n",
    "            loss_train = loss_func(preds, ys)\n",
    "\n",
    "            # clear grad and backwoard\n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % interval_valid == 0:\n",
    "                # set net state\n",
    "                net.eval()\n",
    "\n",
    "                # TODO: validation\n",
    "                acc_valid, loss_valid = [], []\n",
    "                for jdx, batch_v in enumerate(dataloader_valid):\n",
    "                    xs_v, ys_v = batch_v\n",
    "                    xs_v, ys_v = (xs_v.cuda(), ys_v.cuda()) if use_cuda else (xs_v, ys_v)\n",
    "                    xs_v, ys_v = Variable(xs_v), Variable(ys_v)\n",
    "                    preds_v = net(xs_v)\n",
    "                    loss_valid_raw = loss_func(preds_v, ys_v)\n",
    "                    loss_valid.append(loss_valid_raw.cpu().data.numpy() if use_cuda else loss_valid_raw.data.numpy())\n",
    "                    acc_valid.append(evaludate_acc(preds_v, ys_v)[0])\n",
    "\n",
    "                train_xs_raw, train_ys_raw = (xs_raw.cuda(), ys_raw.cuda()) if use_cuda else (xs_raw, ys_raw)\n",
    "                acc_train = evaludate_acc(net(Variable(train_xs_raw)), Variable(train_ys_raw))[0]\n",
    "                acc_valid = np.mean(acc_valid)\n",
    "                loss_valid = np.mean(loss_valid)\n",
    "                bag_loss['train'].append(loss_train.cpu().data.numpy() if use_cuda else loss_train.data.numpy())\n",
    "                bag_loss['valid'].append(loss_valid)\n",
    "                bag_acc['train'].append(acc_train)\n",
    "                bag_acc['valid'].append(acc_valid)\n",
    "\n",
    "                print('[Epoch: {}/{}] [Batch: {}/{}] [Loss-mean(train): {:.4f}] [Loss-mean(validation): {:.4f}] \\\n",
    "[Acc-mean(train): {:.4f}] [Acc-mean(validation): {:.4f}]'.format(\n",
    "                    epoch, sum_epochs, idx, len(dataloader_train),\n",
    "                    np.mean(bag_loss['train']), \n",
    "                    np.mean(bag_loss['valid']), \n",
    "                    np.mean(bag_acc['train']), \n",
    "                    np.mean(bag_acc['valid'])\n",
    "                ), end='\\r')\n",
    "                \n",
    "    return (bag_loss, bag_acc, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import module needed\n",
    "import copy\n",
    "import datetime\n",
    "import matplotlib.markers as plt_markers\n",
    "\n",
    "\n",
    "# define helper class\n",
    "class CoachTeam(object):\n",
    "    \n",
    "    def __init__(self, net, optimizer, loss_func, \n",
    "                 stage_names=['train', 'valid', 'test'], ind_names=['loss', 'acc']):\n",
    "        # net components\n",
    "        self.net = copy.deepcopy(net)\n",
    "        assert hasattr(self.net, 'name'), 'The nn object you want to use should have \\\n",
    "an attribute called `name`.'\n",
    "        self.optimizer = optimizer(self.net.parameters())\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "        # storage\n",
    "        self.stage_names = stage_names\n",
    "        self.ind_names = ind_names\n",
    "        self.results = {ind:{stage:[] for stage in self.stage_names} for ind in self.ind_names}\n",
    "        \n",
    "    def pipeline_helper(self, n_iters, digit_rangesss, sum_epochs=15, interval_valid=50, dirname_prefix='outputs'):\n",
    "        # set dirname\n",
    "        self.dirname = dirname_prefix + '_' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "        if not os.path.isdir(self.dirname):\n",
    "            os.mkdir(self.dirname)\n",
    "        for i in range(n_iters):\n",
    "            # reset data for each experiment\n",
    "            net = copy.deepcopy(self.net)\n",
    "            self.results = {ind:{stage:[] for stage in self.stage_names} for ind in self.ind_names}\n",
    "            \n",
    "            # working pipeline\n",
    "            self.train_helper(net, self.loss_func, self.optimizer, digit_rangesss, \n",
    "                              sum_epochs=sum_epochs, interval_valid=interval_valid)\n",
    "            self.test_helper(self.net)\n",
    "            self.save(mode='file', netname=net.name, suffix=str(i))\n",
    "            num_valid = sum_epochs * len(dataloader_train) // interval_valid\n",
    "            self.plot_single(net.name, plot_test=False, suffix=str(i), num_valid=num_valid)\n",
    "            self.plot_single(net.name, plot_test=True, suffix=str(i))\n",
    "            print('Round {}: OK'.format(i))\n",
    "            print('---------------------------------')\n",
    "        print('==================================================')\n",
    "        self.plot_whole(net.name, n_iters=n_iters, plot_test=False, plot_origin=True, plot_mean=True, num_valid=num_valid)\n",
    "        self.plot_whole(net.name, n_iters=n_iters, plot_test=False, plot_origin=True, plot_mean=False, num_valid=num_valid)\n",
    "        self.plot_whole(net.name, n_iters=n_iters, plot_test=False, plot_origin=False, plot_mean=True, num_valid=num_valid)\n",
    "        self.plot_whole(net.name, n_iters=n_iters, plot_test=True, plot_origin=True, plot_mean=True)\n",
    "        self.plot_whole(net.name, n_iters=n_iters, plot_test=True, plot_origin=True, plot_mean=False)\n",
    "        self.plot_whole(net.name, n_iters=n_iters, plot_test=True, plot_origin=False, plot_mean=True)\n",
    "        print('Finish all the pipeline.')    \n",
    "        \n",
    "        \n",
    "    def train_helper(self, net, loss_func, optimizer, digit_rangesss, sum_epochs=15, interval_valid=50):\n",
    "        # train and validate, using digit_range sequences\n",
    "        for idx,digit_range in enumerate(digit_rangesss):\n",
    "            raw_out = {ind:[] for ind in self.ind_names}\n",
    "\n",
    "            raw_out['loss'], raw_out['acc'], net = train_net(\n",
    "                net=net, loss_func=loss_func, optimizer=optimizer, digit_range=digit_range,\n",
    "                sum_epochs=sum_epochs//len(digit_rangesss), interval_valid=interval_valid\n",
    "            )\n",
    "            \n",
    "            # save in RAM: train_data and valid_data \n",
    "            self.save(mode='dict-results', data=raw_out)\n",
    "            print()\n",
    "            print('Save data(train & validation): OK in RAM, for', digit_range)\n",
    "        print('Train net: OK')\n",
    "        \n",
    "    def test_helper(self, net):\n",
    "        # test(evaluate)\n",
    "        net.eval()\n",
    "        \n",
    "        raw_out = {ind:{'test':[]} for ind in self.ind_names}\n",
    "        for kdx, batch_t in enumerate(dataloader_test):\n",
    "            xs_t, ys_t = batch_t\n",
    "            xs_t, ys_t = (xs_t.cuda(), ys_t.cuda()) if use_cuda else (xs_t, ys_t)\n",
    "            xs_t, ys_t = Variable(xs_t), Variable(ys_t)\n",
    "            preds_t = net(xs_t)\n",
    "            loss_test_raw = loss_func(preds_t, ys_t)\n",
    "            raw_out['loss']['test'].append(loss_test_raw.cpu().data.numpy() if use_cuda \\\n",
    "                                           else loss_test_raw.data.numpy())\n",
    "            raw_out['acc']['test'].append(evaludate_acc(preds_t, ys_t)[0])\n",
    "        \n",
    "        # save in RAM: test_data\n",
    "        self.save(mode='dict-results', data=raw_out, stages=['test'])\n",
    "        print('Save data(test): OK in RAM')\n",
    "        \n",
    "    def save(self, mode, data=None, netname=None, stages=['train', 'valid'], suffix=None):        \n",
    "        if 'dict-results' == mode:\n",
    "            assert data is not None, 'Your should specify the data you want to save in RAM.'\n",
    "            for ind in self.ind_names:\n",
    "                for stage in stages:\n",
    "                    items = [i[0] if isinstance(i, np.ndarray) else i for i in data[ind][stage]]\n",
    "                    if stage not in self.results[ind]:\n",
    "                        self.results[ind][stage] = copy.deepcopy(items)\n",
    "                    else:\n",
    "                        self.results[ind][stage].extend(items)\n",
    "        \n",
    "        if 'file' == mode:\n",
    "            assert netname is not None, 'You should specify the **NAME** of this neural network.'\n",
    "            \n",
    "            lacks = []\n",
    "            for stage in self.stage_names:\n",
    "                tmp_pd = {ind:copy.deepcopy(self.results[ind][stage]) for ind in self.ind_names}\n",
    "                tmp_pd = pd.DataFrame(data=tmp_pd, columns=self.ind_names)\n",
    "                if suffix:\n",
    "                    filename = os.path.join(os.getcwd(), self.dirname, \n",
    "                                            '{}-sheet-{}-{}.csv'.format(netname, stage, suffix))\n",
    "                else:\n",
    "                    filename = os.path.join(os.getcwd(), self.dirname, \n",
    "                                            '{}-sheet-{}.csv'.format(netname, stage))\n",
    "                    \n",
    "                # write to csv\n",
    "                if os.path.isfile(filename):\n",
    "                    pd_source = pd.read_csv(filename)\n",
    "                    tmp_pd = pd.concat([pd_source, tmp_pd])\n",
    "                    tmp_pd.to_csv(filename, index=False) \n",
    "                else:\n",
    "                    tmp_pd.to_csv(filename, index=False)\n",
    "                        \n",
    "                print('Save data in file OK:', filename)\n",
    "    \n",
    "    def plot_single(self, netname, suffix=None, plot_test=True, num_valid=None):        \n",
    "        if plot_test:\n",
    "            # plot: test\n",
    "            \n",
    "            x_inds = np.arange(len(dataloader_test))\n",
    "            fig, axes_raw = plt.subplots(1, len(self.ind_names), figsize=(15,5))\n",
    "            axes = {ind:axes_raw[i] for i,ind in enumerate(self.ind_names)}\n",
    "            for ind in self.ind_names:\n",
    "                axes[ind].plot(x_inds, self.results[ind]['test'], label='{}_test'.format(ind))\n",
    "                axes[ind].set_xlabel('num_iter')\n",
    "                axes[ind].set_ylabel(ind)\n",
    "            \n",
    "            if suffix:\n",
    "                filename = os.path.join(os.getcwd(), self.dirname, \n",
    "                                        '{}-curves_test-{}'.format(netname, suffix))\n",
    "            else:\n",
    "                filename = os.path.join(os.getcwd(), self.dirname, '{}-curves_test'.format(netname))\n",
    "                \n",
    "            plt.savefig(filename)\n",
    "            \n",
    "        if not plot_test:\n",
    "            # plot: train & validation\n",
    "            \n",
    "            fig, axes_raw = plt.subplots(1, len(self.ind_names), figsize=(15, 5))\n",
    "            axes = {self.ind_names[i]:ax for i,ax in enumerate(axes_raw) }\n",
    "            \n",
    "            assert num_valid is not None, 'You should specify how many times \\\n",
    "the validation process executed each round.'\n",
    "            x_inds = np.arange(num_valid)\n",
    "            for ind in ['loss', 'acc']:\n",
    "                for stage in ['train', 'valid']:\n",
    "                    axes[ind].plot(x_inds, self.results[ind][stage], label='{}_{}'.format(ind, stage))\n",
    "                \n",
    "                axes[ind].set_xlabel('num_iter')\n",
    "                axes[ind].set_ylabel(ind)\n",
    "                axes[ind].set_title('data on: convnet_0')\n",
    "                axes[ind].legend()\n",
    "            \n",
    "            plt.tight_layout(w_pad=10)\n",
    "            \n",
    "            if suffix:\n",
    "                filename = os.path.join(os.getcwd(), self.dirname, \n",
    "                                        '{}-curves-train_validation-{}'.format(netname, suffix))\n",
    "            else:\n",
    "                filename = os.path.join(os.getcwd(), self.dirname, '{}-curves-train_validation'.format(netname))\n",
    "                \n",
    "            plt.savefig(filename)\n",
    "\n",
    "            # # fig.show() \n",
    "            # comment the command above to avoid warning \n",
    "            #`UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure`\n",
    "            \n",
    "    def plot_whole(self, netname, n_iters, num_valid=None, plot_test=False, plot_origin=True, plot_mean=False):\n",
    "        empty_markers = ['None', None, ' ', '']\n",
    "        plt_markers_list = list([mk for mk in plt_markers.MarkerStyle.markers.keys() if mk not in empty_markers])\n",
    "        plt_markers_gen = (mk for mk in plt_markers_list)\n",
    "        \n",
    "        fig, axes_raw = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        axes = {ind:axes_raw[i] for i,ind in enumerate(self.ind_names)}\n",
    "        alpha_dict = {'train':0.7 ,'valid':1.0, 'test':1.0}\n",
    "\n",
    "        ind_data = {}\n",
    "        for ind in self.ind_names:\n",
    "            for stage in self.stage_names:\n",
    "                k = '{}_{}-mean'.format(ind, stage)\n",
    "                ind_data[k] = []\n",
    "                \n",
    "        if plot_test:\n",
    "            x_inds = np.arange(len(dataloader_test))\n",
    "            \n",
    "            for i in range(n_iters):\n",
    "                for stage in self.stage_names[-1:]:\n",
    "                    filename = '{}-sheet-{}-{}.csv'.format(netname, stage, i)\n",
    "                    filename = os.path.join(os.getcwd(), self.dirname, filename)\n",
    "                    df = pd.read_csv(filename)\n",
    "                    for ind in self.ind_names:\n",
    "                        # save data for the mean curve\n",
    "                        k = '{}_{}-mean'.format(ind, stage)\n",
    "                        ind_data[k].append(df[ind].values)\n",
    "                        \n",
    "                        # plot\n",
    "                        if plot_origin:\n",
    "                            axes[ind].plot(x_inds, df[ind], label='{}_{}-{}'.format(ind, stage, i), \n",
    "                                           marker=next(plt_markers_gen), alpha=alpha_dict[stage])\n",
    "            if plot_mean:\n",
    "                for ind in self.ind_names:\n",
    "                    for stage in self.stage_names[-1:]:\n",
    "                        k = '{}_{}-mean'.format(ind, stage)\n",
    "                        ind_data[k] = np.array(ind_data[k]).mean(axis=0)\n",
    "                        axes[ind].plot(x_inds, ind_data[k], label=k,\n",
    "                                       marker=next(plt_markers_gen), alpha=alpha_dict[stage])\n",
    "        \n",
    "            for ind in self.ind_names:    \n",
    "                axes[ind].legend()\n",
    "\n",
    "            plt.tight_layout(w_pad=10)\n",
    "\n",
    "            use_origin = 'Origin' if plot_origin else 'noOrigin'\n",
    "            use_mean = 'Mean' if plot_origin else 'noMean'\n",
    "            filename = '{}-curves-test-whole-{}{}'.format(netname, use_origin, use_mean)\n",
    "            filename = os.path.join(os.getcwd(), self.dirname, filename)\n",
    "            plt.savefig(filename)\n",
    "                \n",
    "        if not plot_test:\n",
    "            assert num_valid is not None, \"You must specify the num of iterations on validation.\"\n",
    "            # plot: train_validation\n",
    "            x_inds = np.arange(num_valid)\n",
    "            \n",
    "            for i in range(n_iters):\n",
    "                for stage in self.stage_names[:-1]:\n",
    "                    filename_base = '{}-sheet-{}-{}.csv'.format(netname, stage, i)\n",
    "                    filename = os.path.join(os.getcwd(), self.dirname, filename_base)\n",
    "                    assert os.path.isfile(filename), 'Lack file: '.format(filename_base)\n",
    "                    df = pd.read_csv(filename)\n",
    "                    for ind in self.ind_names:\n",
    "                        # save data for the mean curve\n",
    "                        k = '{}_{}-mean'.format(ind, stage)\n",
    "                        ind_data[k].append(df[ind].values)\n",
    "        \n",
    "                        # plot\n",
    "                        if plot_origin:\n",
    "                            axes[ind].plot(x_inds, df[ind], label='{}_{}-{}'.format(ind, stage, i), \n",
    "                                           marker=next(plt_markers_gen), alpha=alpha_dict[stage])\n",
    "\n",
    "            if plot_mean:\n",
    "                for ind in self.ind_names:\n",
    "                    for stage in self.stage_names[:-1]:\n",
    "                        k = '{}_{}-mean'.format(ind, stage)\n",
    "                        ind_data[k] = np.array(ind_data[k]).mean(axis=0)\n",
    "                        axes[ind].plot(x_inds, ind_data[k], label=k,\n",
    "                                       marker=next(plt_markers_gen), alpha=alpha_dict[stage])\n",
    "\n",
    "            for ind in self.ind_names:    \n",
    "                axes[ind].legend()\n",
    "            \n",
    "            plt.tight_layout(w_pad=10)\n",
    "            \n",
    "            use_origin = 'Origin' if plot_origin else 'noOrigin'\n",
    "            use_mean = 'Mean' if plot_origin else 'noMean'\n",
    "            filename = '{}-curves-train_validation-whole-{}{}'.format(netname, use_origin, use_mean)\n",
    "            filename = os.path.join(os.getcwd(), self.dirname, filename)\n",
    "            plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ConvNet_0 [0~4 -> 5~9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "报错过 `Assertion `cur_target >= 0 && cur_target < n_classes' failed.`，于是打印预测值，发现预测值是\n",
    "\n",
    "```\n",
    "0.1296\n",
    "-0.0020\n",
    "-0.0002\n",
    "-0.0004\n",
    " 0.0679\n",
    " 0.0892\n",
    "-0.0001\n",
    "-0.0057\n",
    "-0.0023\n",
    "-0.0033\n",
    " 0.4178\n",
    "-0.0057\n",
    " 0.3687\n",
    "-0.0001\n",
    " 0.4879\n",
    "-0.0012\n",
    "-0.0029\n",
    " 0.4439\n",
    "-0.0038\n",
    "-0.0007\n",
    "-0.0016\n",
    " 0.0780\n",
    "-0.0021\n",
    "-0.0010\n",
    "-0.0016\n",
    "-0.0034\n",
    " 0.2390\n",
    "-0.0039\n",
    "-0.0032\n",
    "-0.0005\n",
    "-0.0031\n",
    "-0.0033\n",
    "-0.0007\n",
    " 1.0161\n",
    "-0.0013\n",
    "-0.0033\n",
    " 0.1488\n",
    "```\n",
    "\n",
    "于是我估计前向传播出问题了，看了一下 `forward`，果然，由于延续了之前的代码，没有改成 softmax\n",
    "\n",
    "\n",
    "后来又报错 `Expected 2 or 4 dimensions (got 1) `，仔细查看网络结构，对比现有网络和之前成功的网络，发现自己通不过的网络里，最后一层全连接是 1 个结点，想到估计是这个导致 softmax 没法输出（本应接受 `n_class` 个输入才能有对应输出的）\n",
    "\n",
    "后来报错 `inconsistent tensor size, expected r_ [1000], ta [1000] and tb [100] to have the same number of elements, but got 1000, 1000 and 100 elements respectively at /opt/conda/conda-bld/pytorch_1502004572321/work/torch/lib/TH/generic/THTensorMath.c:2829`，然后我估计是形状不匹配，于是在测试前加入形状代码，发现现在一个输出是 10 维度，一个原始数据是 1 维，当然无法比较。那么为什么原来的网络可以呢？对比后发现，原来的网络在测试时使用了 `np.argmax` 来转换。于是我就想，干脆在网络输出前用 `torch.max` 来取值就好了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####\n",
    "##需要在每次做新实验时重新初始化一个新的卷积网络\n",
    "##否则就相当于只是多了几个 epoch 而已，起点不同，无法比较\n",
    "\n",
    "## 每次实验重新初始化时也得清空保存结果所用的数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 49/50] [Batch: 550/600] [Loss-mean(train): 2.3190] [Loss-mean(validation): 2.2961] [Acc-mean(train): 0.1259] [Acc-mean(validation): 0.1466]\n",
      "Save data(train & validation): OK in RAM, for range(0, 5)\n",
      "[Epoch: 49/50] [Batch: 550/600] [Loss-mean(train): 2.3210] [Loss-mean(validation): 2.2961] [Acc-mean(train): 0.1254] [Acc-mean(validation): 0.1466]\n",
      "Save data(train & validation): OK in RAM, for range(0, 5)\n",
      "[Epoch: 49/50] [Batch: 550/600] [Loss-mean(train): 2.3084] [Loss-mean(validation): 2.2962] [Acc-mean(train): 0.1246] [Acc-mean(validation): 0.1462]\n",
      "Save data(train & validation): OK in RAM, for range(5, 10)\n",
      "Train net: OK\n",
      "Save data(test): OK in RAM\n",
      "Save data in file OK: /output/outputs_170908-175215/net00-sheet-train-5.csv\n",
      "Save data in file OK: /output/outputs_170908-175215/net00-sheet-valid-5.csv\n",
      "Save data in file OK: /output/outputs_170908-175215/net00-sheet-test-5.csv\n",
      "Round 5: OK\n",
      "---------------------------------\n",
      "[Epoch: 49/50] [Batch: 550/600] [Loss-mean(train): 2.3206] [Loss-mean(validation): 2.2961] [Acc-mean(train): 0.1257] [Acc-mean(validation): 0.1463]\n",
      "Save data(train & validation): OK in RAM, for range(0, 5)\n",
      "[Epoch: 49/50] [Batch: 550/600] [Loss-mean(train): 2.3078] [Loss-mean(validation): 2.2962] [Acc-mean(train): 0.1251] [Acc-mean(validation): 0.1461]\n",
      "Save data(train & validation): OK in RAM, for range(5, 10)\n",
      "Train net: OK\n",
      "Save data(test): OK in RAM\n",
      "Save data in file OK: /output/outputs_170908-175215/net00-sheet-train-6.csv\n",
      "Save data in file OK: /output/outputs_170908-175215/net00-sheet-valid-6.csv\n",
      "Save data in file OK: /output/outputs_170908-175215/net00-sheet-test-6.csv\n",
      "Round 6: OK\n",
      "---------------------------------\n",
      "[Epoch: 49/50] [Batch: 550/600] [Loss-mean(train): 2.3203] [Loss-mean(validation): 2.2961] [Acc-mean(train): 0.1253] [Acc-mean(validation): 0.1463]\n",
      "Save data(train & validation): OK in RAM, for range(0, 5)\n",
      "[Epoch: 49/50] [Batch: 550/600] [Loss-mean(train): 2.3086] [Loss-mean(validation): 2.2962] [Acc-mean(train): 0.1240] [Acc-mean(validation): 0.1457]\n",
      "Save data(train & validation): OK in RAM, for range(5, 10)\n",
      "Train net: OK\n",
      "Save data(test): OK in RAM\n",
      "Save data in file OK: /output/outputs_170908-175215/net00-sheet-train-7.csv\n",
      "Save data in file OK: /output/outputs_170908-175215/net00-sheet-valid-7.csv\n",
      "Save data in file OK: /output/outputs_170908-175215/net00-sheet-test-7.csv\n",
      "Round 7: OK\n",
      "---------------------------------\n",
      "[Epoch: 24/50] [Batch: 0/600] [Loss-mean(train): 2.3198] [Loss-mean(validation): 2.2961] [Acc-mean(train): 0.1229] [Acc-mean(validation): 0.1463]3]\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# init\n",
    "\n",
    "net00 = ConvNet('net00').cuda() if use_cuda else ConvNet('net00')\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam# (net00.parameters())\n",
    "\n",
    "coach00 = CoachTeam(net00, optimizer, loss_func)\n",
    "\n",
    "coach00.pipeline_helper(\n",
    "    n_iters=10, \n",
    "    digit_rangesss=[range(4+1), range(4+1, 9+1)],\n",
    "    sum_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blabla..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ConvNet_1 [0~4 -> 0~9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# init\n",
    "\n",
    "net01 = ConvNet('net01').cuda() if use_cuda else ConvNet('net01')\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam# (net00.parameters())\n",
    "\n",
    "coach01 = CoachTeam(net01, optimizer, loss_func)\n",
    "\n",
    "coach01.pipeline_helper(\n",
    "    n_iters=10, \n",
    "    digit_rangesss=[range(4+1), range(9+1)],\n",
    "    sum_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blabla..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ConvNet_2 [0~9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# init\n",
    "\n",
    "net02 = ConvNet('net02').cuda() if use_cuda else ConvNet('net02')\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam# (net00.parameters())\n",
    "\n",
    "coach02 = CoachTeam(net02, optimizer, loss_func)\n",
    "\n",
    "coach02.pipeline_helper(\n",
    "    n_iters=10, \n",
    "    digit_rangesss=[range(9+1)],\n",
    "    sum_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blabla..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
